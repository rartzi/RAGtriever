[vault]
root = "/path/to/vault"
ignore = [
    ".git/**",
    ".obsidian/cache/**",
    "**/.DS_Store",
    "**/~$*",           # Office temp files (Word, Excel, PowerPoint)
    "**/.~lock.*",      # LibreOffice lock files
    "**/~*.tmp",        # Generic temp files
    "**/*.swp",         # Vim swap files
    "**/*.bak"          # Backup files
]

[index]
dir = "~/.ragtriever/indexes/myvault"
extractor_version = "v1"
chunker_version = "v2"  # v2: adds chunk overlap support

[chunking]
# Overlap between adjacent chunks (in characters) for context preservation
# Setting this to 0 disables overlap (classic behavior)
overlap_chars = 200

# Maximum chunk size before splitting (for large sections/documents)
max_chunk_size = 2000

# Preserve heading hierarchy in metadata even when chunks are split
preserve_heading_metadata = true

[embeddings]
provider = "sentence_transformers"
model = "BAAI/bge-small-en-v1.5"
batch_size = 32
device = "cpu"  # cpu|cuda|mps
# Set to true to use cached models only (no HuggingFace downloads)
# Can also be controlled via HF_OFFLINE_MODE environment variable
# IMPORTANT: Run once with offline_mode=false to download the model first,
# then switch to offline_mode=true. Check cached models: ls ~/.cache/huggingface/hub/
offline_mode = true

# Query instruction prefix for asymmetric retrieval (improves relevance)
# Enabled by default for BGE models
use_query_prefix = true
query_prefix = "Represent this sentence for searching relevant passages: "

# FAISS approximate nearest neighbor index (for large vaults)
# Performance: ~10-20ms vs ~50-100ms+ for brute-force at 10K chunks
# Recommendation:
#   - Keep false for <5K chunks (brute-force is fast enough)
#   - Enable for >10K chunks (significant speedup)
# Install with: pip install faiss-cpu (or faiss-gpu for CUDA)
use_faiss = false  # Default: false (enable when vault grows large)
faiss_index_type = "IVF"  # "Flat" (exact), "IVF" (fast), "HNSW" (fastest)
faiss_nlist = 100  # Number of clusters (IVF only)
faiss_nprobe = 10  # Number of clusters to search (IVF only, higher = more accurate but slower)

[image_analysis]
provider = "tesseract"  # tesseract|gemini|vertex_ai|aigateway|off
# Automatically extracts and analyzes:
# - Images embedded in PDFs (extracted via PyMuPDF)
# - Images in PowerPoint slides (extracted via python-pptx)
# - Images referenced in Markdown (![](path) and ![[image]])
# - Standalone image files (.png, .jpg, .jpeg, .webp, .gif)
#
# Provider-specific requirements:
#   tesseract:  pip install pytesseract + system tesseract-ocr
#   gemini:     Set GEMINI_API_KEY env var (or gemini_api_key below)
#   vertex_ai:  Configure [vertex_ai] section below
#   aigateway:  Configure [aigateway] section below
#   off:        No configuration needed
#
# gemini_api_key = ""  # or set GEMINI_API_KEY env var (recommended)
gemini_model = "gemini-2.0-flash"  # Default for gemini/aigateway providers

# Resilience settings (for API-based providers: gemini, vertex_ai, aigateway)
# Prevents scans from hanging on unresponsive APIs with automatic retry and circuit breaker
timeout = 60000           # Request timeout in milliseconds (default: 60s)
max_retries = 3           # Retry transient errors up to 3x (default: 3)
retry_backoff = 1000      # Base backoff 1s, doubles each retry (default: 1000ms)
circuit_threshold = 5     # Trip breaker after N consecutive failures (default: 5)
circuit_reset = 60        # Auto-reset breaker after N seconds (default: 60)

# Vertex AI configuration (for provider = "vertex_ai")
[vertex_ai]
# project_id = "your-gcp-project-id"  # or set GOOGLE_CLOUD_PROJECT env var
# location = "global"  # or us-central1, us-east4, etc. depending on model availability
# credentials_file = "/path/to/service-account.json"  # or set GOOGLE_APPLICATION_CREDENTIALS env var
# model = "gemini-2.0-flash-exp"  # available models depend on region

# Microsoft AI Gateway configuration (for provider = "aigateway")
# Enables enterprise access to Gemini models via Microsoft AI Gateway
[aigateway]
# url = "https://your-gateway.azure.com"  # or set AI_GATEWAY_URL env var
# key = "your-api-key"  # or set AI_GATEWAY_KEY env var
# model = "gemini-2.5-pro"    # Best quality (slower, recommended for important images)
# model = "gemini-2.5-flash"  # Fast & good (default, recommended for batch processing)
# timeout = 60000  # Request timeout in milliseconds (default: 60s, uses [image_analysis].timeout if not set)
# endpoint_path = "vertex-ai-express"  # Path suffix appended to URL (default: vertex-ai-express)

[retrieval]
k_vec = 40
k_lex = 40
top_k = 10
use_rerank = false

# Cross-Encoder Reranking (Optional)
# Improves result quality by 20-30% at the cost of ~100-200ms latency
# Reranks top candidates from hybrid search using a cross-encoder model
#
# Performance:
#   CPU: ~100-200ms for 40 candidates
#   GPU: ~20-50ms for 40 candidates
#
# When to enable:
#   - You want best possible result quality
#   - Getting false positives in search results
#   - Latency <300ms is acceptable
#
# rerank_model = "cross-encoder/ms-marco-MiniLM-L-6-v2"  # Default (balanced)
# rerank_device = "cpu"  # cpu|cuda|mps
# rerank_top_k = 10      # Number of results after reranking
#
# Other models:
#   "cross-encoder/ms-marco-TinyBERT-L-2-v2" - Fast (50-80ms, 17MB)
#   "cross-encoder/ms-marco-MiniLM-L-6-v2"   - Balanced (100-200ms, 80MB) âœ… Default
#   "cross-encoder/ms-marco-electra-base"    - Accurate (200-400ms, 400MB)

# Fusion Algorithm
# Controls how vector and lexical search results are merged
#
# "rrf" (default): Reciprocal Rank Fusion - robust rank-based fusion
#   score = sum(1 / (k + rank_i)) for each list containing the doc
#   Doesn't require score calibration, works well across different scales
#
# "weighted": Score-based weighted average
#   score = vec_weight * vec_score + lex_weight * lex_score
#   Requires normalized scores, sensitive to score distributions
#
fusion_algorithm = "rrf"
rrf_k = 60  # RRF constant (higher = more uniform weighting across ranks)

# Backlink Boost
# Boosts documents that have more incoming links from other documents
# Implements a simplified PageRank-like authority signal
#
# How it works:
#   - Each backlink adds backlink_boost_weight to the document score
#   - Maximum boost = backlink_boost_weight * backlink_boost_cap
#   - Example: 5 backlinks with weight=0.1 adds 0.5 (50%) to the score
#
# When to enable:
#   - Knowledge bases where well-linked notes are more authoritative
#   - Zettelkasten-style vaults with dense cross-references
#   - When important "hub" documents should rank higher
#
backlink_boost_enabled = true
backlink_boost_weight = 0.1  # Score boost per backlink (10%)
backlink_boost_cap = 10      # Maximum backlinks counted (caps boost at 100%)

# Recency Boost
# Boosts recently modified documents, decaying with age
# Useful when fresh information is more relevant
#
# How it works:
#   - Files modified within fresh_days: full boost (e.g., +20%)
#   - Files modified within recent_days: partial boost (e.g., +10%)
#   - Files modified within old_days: no change
#   - Files older than old_days: slight penalty (e.g., -5%)
#
# When to enable:
#   - Active projects where recent edits matter more
#   - News/research vaults where freshness indicates relevance
#   - Disable for reference material where age doesn't matter
#
recency_boost_enabled = true
recency_fresh_days = 14   # Files this recent get max boost
recency_recent_days = 60  # Files this recent get medium boost
recency_old_days = 180    # Files older than this get slight penalty

# Heading/Title Boost (DISABLED by default)
# Boosts chunks that are document titles or section headings (H1, H2, H3)
#
# WARNING: Can create file-type bias favoring markdown over PDFs/PPTX
# Content and semantic relevance should generally matter most!
#
# How it works:
#   - H1 (document title): heading_h1_boost multiplier (e.g., 1.05 = +5%)
#   - H2 (major sections): heading_h2_boost multiplier (e.g., 1.03 = +3%)
#   - H3 (subsections): heading_h3_boost multiplier (e.g., 1.02 = +2%)
#   - These multiply with other boosts (can compound unexpectedly!)
#
# When to enable:
#   - You want overviews/summaries to rank higher than details
#   - Your vault has well-structured markdown with clear heading hierarchy
#   - You're okay with markdown files ranking higher than PDFs/presentations
#
# When to DISABLE (default):
#   - Policy/compliance questions where authoritative PDFs matter most
#   - Mixed content (markdown notes + official docs) where file type shouldn't matter
#   - You want pure semantic relevance without structural bias
#
heading_boost_enabled = false
# heading_h1_boost = 1.05  # 5% boost for H1 (title) chunks
# heading_h2_boost = 1.03  # 3% boost for H2 chunks
# heading_h3_boost = 1.02  # 2% boost for H3 chunks

# Tag Boost (DISABLED by default)
# Boosts chunks from documents whose tags match query terms
#
# WARNING: Can create file-type bias favoring tagged markdown over PDFs/PPTX
# Content and semantic relevance should generally matter most!
#
# How it works:
#   - For each tag matching a query term: adds tag_boost_weight
#   - Maximum boost = tag_boost_weight * tag_boost_cap
#   - Tags normalized: #machine-learning matches "machine learning" query
#   - Multiplies with other boosts (can compound unexpectedly!)
#
# When to enable:
#   - Your vault uses consistent, meaningful tags
#   - Tags indicate topical relevance (e.g., #ai-gateway, #kubernetes)
#   - You want tagged notes to rank slightly higher
#
# When to DISABLE (default):
#   - Mixed content where only some files have tags
#   - You want pure semantic relevance without tag bias
#   - Official docs (PDFs) don't have tags but are authoritative
#
tag_boost_enabled = false
# tag_boost_weight = 0.03  # 3% boost per matching tag
# tag_boost_cap = 3        # Max 3 tags counted (caps at 9% boost)

# Result Diversity (MMR - Maximal Marginal Relevance)
# Limits chunks per document to ensure diverse results
#
# How it works:
#   - Returns at most max_per_document chunks from each document
#   - Fills remaining slots from other documents
#   - Prevents one highly-relevant doc from dominating all results
#
# When to enable (default):
#   - You want to see information from multiple sources
#   - Prevent result redundancy (many chunks from same file)
#
# When to disable:
#   - You want all top-k results regardless of source
#   - Single document deep-dive scenarios
#
diversity_enabled = true
max_per_document = 2  # Maximum chunks from same document

# Parallelization (3.6x speedup for full scans)
# Parallel scanning is enabled by default
[indexing]
# Scan mode settings (ragtriever scan --full)
extraction_workers = 8    # Parallel file extraction workers (default: 8)
embed_batch_size = 256    # Cross-file embedding batch size (GPU efficiency)
image_workers = 8         # Parallel image API workers (default: 8)
parallel_scan = true      # Enable/disable parallel scanning
# CLI overrides: --workers N, --no-parallel

# Watch mode settings (ragtriever watch)
# Batched mode is default - accumulates filesystem events for efficient batch processing
watch_workers = 4         # Parallel extraction workers for watch (default: 4)
watch_batch_size = 10     # Max files per batch before processing (default: 10)
watch_batch_timeout = 5.0 # Seconds before processing partial batch (default: 5.0)
watch_image_workers = 4   # Parallel image workers for watch (default: 4)
# CLI overrides: --batch-size N, --batch-timeout N, --no-batch (for legacy serial mode)

[mcp]
transport = "stdio"

# Logging (audit trail for scan and watch operations)
[logging]
# Log directory (relative to current working directory or absolute path)
dir = "logs"

# Log file paths with date/time patterns
# Supported placeholders:
#   {date}     = YYYYMMDD (e.g., 20260123)
#   {datetime} = YYYYMMDD_HHMMSS (e.g., 20260123_142030)
scan_log_file = "logs/scan_{date}.log"       # Scan log file pattern
watch_log_file = "logs/watch_{datetime}.log" # Watch log file pattern

# Log level: DEBUG (verbose), INFO (default), WARNING, ERROR
level = "INFO"

# Auto-enable logging (if true, logging always enabled; if false, use --log-file CLI flag)
enable_scan_logging = false   # Set to true to always log scan operations
enable_watch_logging = true   # Set to true to always log watch operations (recommended)

# CLI overrides: --log-file PATH, --log-level LEVEL, --verbose (DEBUG)
# Example:  ragtriever scan --log-file logs/my_scan.log --verbose
