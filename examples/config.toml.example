[vault]
root = "/path/to/vault"
ignore = [
    ".git/**",
    ".obsidian/cache/**",
    "**/.DS_Store",
    "**/~$*",           # Office temp files (Word, Excel, PowerPoint)
    "**/.~lock.*",      # LibreOffice lock files
    "**/~*.tmp",        # Generic temp files
    "**/*.swp",         # Vim swap files
    "**/*.bak"          # Backup files
]

[index]
dir = "~/.ragtriever/indexes/myvault"
extractor_version = "v1"
chunker_version = "v2"  # v2: adds chunk overlap support

[chunking]
# Overlap between adjacent chunks (in characters) for context preservation
# Setting this to 0 disables overlap (classic behavior)
overlap_chars = 200

# Maximum chunk size before splitting (for large sections/documents)
max_chunk_size = 2000

# Preserve heading hierarchy in metadata even when chunks are split
preserve_heading_metadata = true

[embeddings]
provider = "sentence_transformers"
model = "BAAI/bge-small-en-v1.5"
batch_size = 32
device = "cpu"  # cpu|cuda|mps
# Set to true to use cached models only (no HuggingFace downloads)
# Can also be controlled via HF_OFFLINE_MODE environment variable
# IMPORTANT: Run once with offline_mode=false to download the model first,
# then switch to offline_mode=true. Check cached models: ls ~/.cache/huggingface/hub/
offline_mode = true

# Query instruction prefix for asymmetric retrieval (improves relevance)
# Enabled by default for BGE models
use_query_prefix = true
query_prefix = "Represent this sentence for searching relevant passages: "

# FAISS approximate nearest neighbor index (for large vaults)
# Performance: ~10-20ms vs ~50-100ms+ for brute-force at 10K chunks
# Recommendation:
#   - Keep false for <5K chunks (brute-force is fast enough)
#   - Enable for >10K chunks (significant speedup)
# Install with: pip install faiss-cpu (or faiss-gpu for CUDA)
use_faiss = false  # Default: false (enable when vault grows large)
faiss_index_type = "IVF"  # "Flat" (exact), "IVF" (fast), "HNSW" (fastest)
faiss_nlist = 100  # Number of clusters (IVF only)
faiss_nprobe = 10  # Number of clusters to search (IVF only, higher = more accurate but slower)

[image_analysis]
provider = "tesseract"  # tesseract|gemini|vertex_ai|off
# For tesseract: requires pytesseract and tesseract-ocr installed on system
# For gemini: set GEMINI_API_KEY env var or gemini_api_key below
# gemini_api_key = ""
gemini_model = "gemini-2.0-flash"

# Vertex AI configuration (for provider = "vertex_ai")
[vertex_ai]
# project_id = "your-gcp-project-id"  # or set GOOGLE_CLOUD_PROJECT env var
# location = "global"  # or us-central1, us-east4, etc. depending on model availability
# credentials_file = "/path/to/service-account.json"  # or set GOOGLE_APPLICATION_CREDENTIALS env var
# model = "gemini-2.0-flash-exp"  # available models depend on region

[retrieval]
k_vec = 40
k_lex = 40
top_k = 10
use_rerank = false

# Cross-Encoder Reranking (Optional)
# Improves result quality by 20-30% at the cost of ~100-200ms latency
# Reranks top candidates from hybrid search using a cross-encoder model
#
# Performance:
#   CPU: ~100-200ms for 40 candidates
#   GPU: ~20-50ms for 40 candidates
#
# When to enable:
#   - You want best possible result quality
#   - Getting false positives in search results
#   - Latency <300ms is acceptable
#
# rerank_model = "cross-encoder/ms-marco-MiniLM-L-6-v2"  # Default (balanced)
# rerank_device = "cpu"  # cpu|cuda|mps
# rerank_top_k = 10      # Number of results after reranking
#
# Other models:
#   "cross-encoder/ms-marco-TinyBERT-L-2-v2" - Fast (50-80ms, 17MB)
#   "cross-encoder/ms-marco-MiniLM-L-6-v2"   - Balanced (100-200ms, 80MB) âœ… Default
#   "cross-encoder/ms-marco-electra-base"    - Accurate (200-400ms, 400MB)

[mcp]
transport = "stdio"
