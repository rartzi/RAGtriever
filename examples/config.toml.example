[vault]
root = "/path/to/vault"
ignore = [
    ".git/**",
    ".obsidian/cache/**",
    "**/.DS_Store",
    "**/~$*",           # Office temp files (Word, Excel, PowerPoint)
    "**/.~lock.*",      # LibreOffice lock files
    "**/~*.tmp",        # Generic temp files
    "**/*.swp",         # Vim swap files
    "**/*.bak"          # Backup files
]

[index]
dir = "~/.ragtriever/indexes/myvault"
extractor_version = "v1"
chunker_version = "v1"

[chunking]
# Overlap between adjacent chunks (in characters) for context preservation
# Setting this to 0 disables overlap (classic behavior)
overlap_chars = 200

# Maximum chunk size before splitting (for large sections/documents)
max_chunk_size = 2000

# Preserve heading hierarchy in metadata even when chunks are split
preserve_heading_metadata = true

[embeddings]
provider = "sentence_transformers"
model = "BAAI/bge-small-en-v1.5"
batch_size = 32
device = "cpu"  # cpu|cuda|mps
# Set to true to use cached models only (no HuggingFace downloads)
# Can also be controlled via HF_OFFLINE_MODE environment variable
# IMPORTANT: Run once with offline_mode=false to download the model first,
# then switch to offline_mode=true. Check cached models: ls ~/.cache/huggingface/hub/
offline_mode = true

# Query instruction prefix for asymmetric retrieval (improves relevance)
# Enabled by default for BGE models
use_query_prefix = true
query_prefix = "Represent this sentence for searching relevant passages: "

# FAISS approximate nearest neighbor index (for large vaults >10K chunks)
# Significantly faster than brute-force for large datasets
# Install with: pip install faiss-cpu (or faiss-gpu for CUDA)
use_faiss = false  # Enable for vaults with >10K chunks
faiss_index_type = "IVF"  # "Flat" (exact), "IVF" (fast), "HNSW" (fastest)
faiss_nlist = 100  # Number of clusters (IVF only)
faiss_nprobe = 10  # Number of clusters to search (IVF only, higher = more accurate but slower)

[image_analysis]
provider = "tesseract"  # tesseract|gemini|vertex_ai|off
# For tesseract: requires pytesseract and tesseract-ocr installed on system
# For gemini: set GEMINI_API_KEY env var or gemini_api_key below
# gemini_api_key = ""
gemini_model = "gemini-2.0-flash"

# Vertex AI configuration (for provider = "vertex_ai")
[vertex_ai]
# project_id = "your-gcp-project-id"  # or set GOOGLE_CLOUD_PROJECT env var
# location = "global"  # or us-central1, us-east4, etc. depending on model availability
# credentials_file = "/path/to/service-account.json"  # or set GOOGLE_APPLICATION_CREDENTIALS env var
# model = "gemini-2.0-flash-exp"  # available models depend on region

[retrieval]
k_vec = 40
k_lex = 40
top_k = 10
use_rerank = false

[mcp]
transport = "stdio"
